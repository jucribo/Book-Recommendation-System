{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf699d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import time\n",
    "import itertools\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Web Scrapping Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dados_iniciais_livro(i):\n",
    "    \n",
    "    # função para obter informações iniciais do livro utilizando a numeração de identificação do livro no site Skoob.\n",
    "    # informações iniciais obtidas:\n",
    "    # Livro, Autor, ISBN-13, lançamento, páginas do livro, total de resenhas, url onde se encontram as resenha\n",
    "    \n",
    "    url_resenha = 'https://www.skoob.com.br/livro/resenhas/{}/mais-gostaram/mpage:1'.format(i) \n",
    "    # página do Skoob em que constam as resenhas, em que o primeiro {} será o número do livro e\n",
    "    # o segundo {} a página de reviews\n",
    "    \n",
    "    response = requests.get(url_resenha.format(i))\n",
    "    pagina_html = BeautifulSoup(response.text)\n",
    "    corpo = pagina_html.findAll(id=\"corpo\")[0] # corpo da página a ser feita a raspagem de dados (Web Scraping)    \n",
    "    cabecario = corpo.find(id='pg-livro-menu-principal-container')\n",
    "    \n",
    "    Livro = cabecario.find('strong',{'class':'sidebar-titulo'}).get_text()\n",
    "    \n",
    "    try:\n",
    "        Autor = cabecario.findAll('a')[1].get_text() # Nome do Autor     \n",
    "    except:\n",
    "        Autor = None # Caso não obtenha nome de autor\n",
    "    if Autor == '\\nR$ \\n': # nos casos em que o nome do autor não possui um hiperlink, ele não será encontrado com Find('a')\n",
    "        Autor = cabecario.find('i').get_text()  # nestes casos, um find i seria necessário\n",
    "    elif Autor == '\\n\\n': # outra opção de resultado para casos em que não há hiperlink\n",
    "        Autor = cabecario.find('i').get_text() \n",
    "        \n",
    "    try:\n",
    "        ISBN_13 = cabecario.find('div', {\"class\": \"sidebar-desc\"}).find('span').get_text() # obter ISBN do livro\n",
    "    except:\n",
    "        ISBN_13 = None \n",
    "        \n",
    "    try:\n",
    "        lancamento = int(cabecario.find('div', {\"class\": \"sidebar-desc\"}).get_text().split('Ano: ')[-1].split(' /',1)[0])\n",
    "    except:\n",
    "        lancamento = None\n",
    "    \n",
    "    try:\n",
    "        Paginas = int(cabecario.find('div', {\"class\": \"sidebar-desc\"}).get_text().split('Páginas: ')[-1].split(' ',1)[0])    \n",
    "    except:\n",
    "        Paginas = None\n",
    "        \n",
    "    try: # obter o total de resenhas disponíveis, para identificar o range necessário para obter todas as resenhas\n",
    "        total_resenhas = int(corpo.find('div',{'class' :'contador'}).find('b').get_text().split(' ')[0])\n",
    "    except:\n",
    "        total_resenhas = 0\n",
    "    \n",
    "    print('Livro {}: {}'.format(i, Livro))\n",
    "        \n",
    "    return(Livro, Autor, ISBN_13, lancamento, Paginas, total_resenhas, url_resenha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b44d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detalhes_do_livro(i, Livro):\n",
    "    \n",
    "    # função para obter um maior detalhamento do livro, com informações do skoob\n",
    "    # Descricao, gêneros, Rating médio, Avaliações, total de resenhas, usuários que abandonaram a leitura\n",
    "    # usuários relendo, usuários que querem_ler, usuários lendo, usuários que leram, url do detalhe do livro\n",
    "    \n",
    "    titulo_tratado = Livro # tratar o titulo, retirando pontuações dos titulos, que não são utilizados no link\n",
    "    for c in string.punctuation:\n",
    "        titulo_tratado = titulo_tratado.replace(c,'')\n",
    "    \n",
    "    # link dos detalhes de cada livro é composto pelo titulo, com um hifem entre palavras\n",
    "    # e pelo numero de identificação do livro no site\n",
    "    url_livro = ('https://www.skoob.com.br/'+titulo_tratado+'-'+str(i)).replace(' ','-').lower().replace('--','-')\n",
    "    \n",
    "    response2 = requests.get(url_livro)\n",
    "    pagina_html2 = BeautifulSoup(response2.text)\n",
    "    corpo2 = pagina_html2.find(id=\"corpo\") # corpo da página a ser feita a raspagem de dados (Web Scraping)\n",
    "    detalhes = corpo2.find(id='pg-livro-principal-container')\n",
    "    \n",
    "    Rating = float(detalhes.find(id=\"pg-livro-box-rating\").find('span').get_text())\n",
    "    Avaliacoes = int(detalhes.find(id=\"pg-livro-box-rating-avaliadores-numero\").get_text().split(' ')[0].replace('.',''))\n",
    "    \n",
    "    total_resenhas_real = int(detalhes.findAll('div',{'class':'bar'})[0].findAll('a')[1].get_text().replace('.',''))\n",
    "    abandonos = int(detalhes.findAll('div',{'class':'bar'})[1].findAll('a')[1].get_text().replace('.',''))\n",
    "    relendo = int(detalhes.findAll('div',{'class':'bar'})[2].findAll('a')[1].get_text().replace('.',''))\n",
    "    querem_ler = int(detalhes.findAll('div',{'class':'bar'})[3].findAll('a')[1].get_text().replace('.',''))\n",
    "    lendo = int(detalhes.findAll('div',{'class':'bar'})[4].findAll('a')[1].get_text().replace('.',''))\n",
    "    leram = int(detalhes.findAll('div',{'class':'bar'})[5].findAll('a')[1].get_text().replace('.',''))\n",
    "    \n",
    "    try:\n",
    "        descricao = detalhes.find(id='livro-perfil-sinopse-txt').find('p').get_text()\n",
    "    except:\n",
    "        descricao = None\n",
    "    \n",
    "    try: # procurar encontrar um gênero, caso exista, encontar a descrição \n",
    "        generos = detalhes.find(id='livro-perfil-sinopse-txt').find('span').get_text()\n",
    "        descricao = descricao.split(generos)[0] # remover generos da descrição\n",
    "        generos = generos.strip().split(' / ') # gerar uma lista com os diversos gêneros\n",
    "    except:\n",
    "        generos = None\n",
    "\n",
    "    return(descricao, generos, Rating, Avaliacoes, total_resenhas_real, abandonos, relendo, querem_ler, lendo, leram, url_livro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_de_lancamento(i, lancamento):\n",
    "    \n",
    "    # função para procurar a data de lançamento \"correta\"\n",
    "    # o dado obtido anteriormente não necessariamente é a da primeira edição lançada\n",
    "    # procurar dentre as diversas edições a que tiver a menor data de lançamento\n",
    "    \n",
    "    url_edicoes = 'https://www.skoob.com.br/livro/edicoes/'+str(i)\n",
    "    \n",
    "    response3 = requests.get(url_edicoes)\n",
    "    pagina_html3 = BeautifulSoup(response3.text)\n",
    "    corpo3 = pagina_html3.find(id=\"corpo\").find('div',{'style':'margin-top:10px;'}) # corpo da página a ser feita a raspagem de dados (Web Scraping)\n",
    "    \n",
    "    numero_edicoes = len(corpo3.findAll('div',{'style':'float:left; width:180px;'}))\n",
    "    lancamentos = []\n",
    "    for edicoes in range(0,numero_edicoes):\n",
    "        lancamentos.append(int(corpo3.findAll('div',{'style':'float:left; width:180px;'})[edicoes].get_text().split('Ano: ')[-1].split('Páginas:')[0]))\n",
    "    try:\n",
    "        if lancamento == None: # caso não houvesse dado disponível, obter o mínimo existente\n",
    "            try:\n",
    "                lancamento = min(lancamentos)\n",
    "            except:\n",
    "                pass\n",
    "        elif min(lancamentos) < lancamento: # caso o menor valor disponível seja menor que o atual, substituir\n",
    "            lancamento = min(lancamentos)\n",
    "    except: \n",
    "        pass\n",
    "        \n",
    "    return(lancamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4807bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_resenhas(i, Livro, total_resenhas, limite_resenhas, imprimir):\n",
    "    \n",
    "    lista_reviews = [] # lista com output de todas as reviews coletadas\n",
    "    \n",
    "    url_resenha = 'https://www.skoob.com.br/livro/resenhas/{}/mais-gostaram/mpage:{}'\n",
    "    \n",
    "    total_pags = math.ceil(total_resenhas/15)\n",
    "    \n",
    "    if limite_resenhas == 0: # se limite for definido como 0, pegará todas os comentários existentes\n",
    "        pass   \n",
    "    elif total_pags > math.ceil(limite_resenhas/15): # se houverem mais páginas que as necessárias para encontrar o limite\n",
    "        total_pags = math.ceil(limite_resenhas/15) # limitará as páginas de modo a limitar o total de comentários\n",
    "    else:\n",
    "        pass # se limite_resenhas não for 0 nem maior que o existente, teremos menos reviews que o desejado, portanto pegaremos todos os disponíveis\n",
    "    \n",
    "    contador_reviews  = 0 # definindo um contador de comentários por livro\n",
    "        \n",
    "    for paginas_de_resenha in range(1,total_pags+1): # range das páginas com comentários, seguindo limite caso tenha sido definido\n",
    "        response4 = requests.get(url_resenha.format(i,paginas_de_resenha))\n",
    "        pagina_html4 = BeautifulSoup(response4.text)\n",
    "        corpo4 = pagina_html4.find(id=\"corpo\") # corpo das reviews de cada página  \n",
    "        \n",
    "        \n",
    "        if imprimir: # se True, irá printar o livro e página que está sendo coletado\n",
    "            print('Livro {}: {} - Página: {}'.format(i, Livro, paginas_de_resenha)) # printando página da review, para identificar casos de bug\n",
    "\n",
    "        # existem até 15 reviews por página, cada uma com o ID da review aparecendo 2x, portanto\n",
    "        # o range padrão de coleta de reviews por página vai de 0 a 29, pulando de 2 em 2\n",
    "        # no entanto, possívelmente ele foi limitado    \n",
    "    \n",
    "        if (paginas_de_resenha == math.ceil(limite_resenhas/15) and limite_resenhas!= 0): # se a página atual for a mesma definida ao limitar o numero de reviews\n",
    "            ultima_review = (limite_resenhas - contador_reviews)*2 # encontrar quantas reviews ainda serão coletadas para atingir limite definido\n",
    "            # multiplicar por 2 uma vez que pularemos de 2 em 2    \n",
    "        else:\n",
    "            ultima_review = 30 # se nao estiver no limite definido, usará o range completo das reviews \n",
    "            \n",
    "        for resenhas in range(0,ultima_review,2):\n",
    "            try: # testar se existe reviews nesta posição, uma vez que se o total não tiver sido limitado,\n",
    "                # na última página de reviews pode haver menos de 15 reviews\n",
    "                if imprimir: # se True, irá printar o livro e página e review que está sendo coletado\n",
    "                    print('Livro {}: {} - Página: {} - Review: {}'.format(i, Livro, paginas_de_resenha,int(resenhas/2+1))) # printando a posição da review, para identificar casos de bug\n",
    "                          \n",
    "                # dados gerais da resenha    \n",
    "                review_pt1 = corpo4.findAll(id=\"perfil-conteudo-intern\")[0].findAll(\"div\", id=lambda value: value and value.startswith(\"resenha\"))[resenhas]\n",
    "                id_resenha = review_pt1.get('id') # ID da review\n",
    "                id_usuario = review_pt1.find('a').get('href').split('/')[-1] # ID do usuário\n",
    "                nota = float(review_pt1.find('star-rating').get('rate')) # Nota dada pelo usuário\n",
    "                           \n",
    "                # dados de texto da resenha\n",
    "                review_pt2 = corpo4.find(id=\"perfil-conteudo-intern\").findAll(\"div\", id=lambda value: value and value.startswith(\"resenha\"))[resenhas+1]\n",
    "                            \n",
    "                resenha = review_pt2.get_text() # obtendo o texto que consta título, data e resenha  \n",
    "                \n",
    "                if 'site: ' in resenha:\n",
    "                    if len(review_pt2.findAll('strong'))==2:\n",
    "                        titulo = None\n",
    "                        resenha = resenha.split('/',2)[-1][4:]\n",
    "                        resenha = resenha.split('site: ')[0]\n",
    "                    elif len(review_pt2.findAll('strong'))==3:\n",
    "                        titulo = review_pt2.findAll('strong')[1].get_text()\n",
    "                        resenha = resenha.split(titulo,1)[-1] \n",
    "                        resenha = resenha.split('site: ')[0]\n",
    "                else:\n",
    "                    try: # caso exista um título, salvar ele separadamente\n",
    "                        titulo = review_pt2.findAll('strong')[1].get_text()\n",
    "                        resenha = resenha.split(titulo,1)[-1] \n",
    "                    except IndexError: # caso não exista um título, salvar None\n",
    "                        titulo = None\n",
    "                        resenha = resenha.split('/',2)[-1][4:]\n",
    "                if resenha == ' ':\n",
    "                    resenha = titulo\n",
    "                    titulo = None              \n",
    "                \n",
    "                contador_reviews+=1\n",
    "                \n",
    "                lista_reviews.append({'numero_do_livro':i,\n",
    "                                      'review_id': id_resenha,\n",
    "                                      'user_id': id_usuario,\n",
    "                                      'pagina_do_skoob' : paginas_de_resenha,\n",
    "                                      'posicao_na_pagina' : (resenhas/2+1),\n",
    "                                      'rating': nota,\n",
    "                                      'review_title': titulo,\n",
    "                                      'review': resenha})\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "    return(lista_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raspar_skoob(livro_inicial, livro_final, limite_resenhas,imprimir):\n",
    "    # livro_inicial: primeiro livro a ser coletado (número inteiro não nulo)\n",
    "    # livro_final: ultimo livro a ser coletado, incluindo-o (sendo necessáriamente um número maior que o do livro_inicial)\n",
    "    # limite_resenhas: limite de reviews a serem coletados. \n",
    "        # Caso o valor seja menor que o número de reviews existentes, ele limitárá o total\n",
    "        # Caso insira um valor maior que o número de reviews disponíveis, ele pegará o máximo existente\n",
    "        # Caso não deseje limitar, inserir o valor 0\n",
    "    # imprimir: se True, irá printar os estágios, se False, não irá printar os estágios\n",
    "    \n",
    "    print (time.asctime(time.localtime(time.time())))\n",
    "    print('\\n')\n",
    "    \n",
    "    contagem = 0 # contagem para definir saída do loop caso não existam 15 livros em sequência, um indício de que chegou ao final dos livros existentes\n",
    "    \n",
    "    lista_livros = [] # lista dos livros coletados    \n",
    "    \n",
    "    \n",
    "    # range da lista de livros, começando no livro_inicial até livro_final, em que será feita a coleta\n",
    "    for i in range(livro_inicial,livro_final+1):     \n",
    "    \n",
    "        if contagem == 15:\n",
    "            print('\\nHá {} tentativas não foi encontrado um livro, portanto, scraping será interrompido!'.format(contagem))\n",
    "            break\n",
    "\n",
    "        while True: # enquanto for true, seguira tentando obter os dados daquele único livro i\n",
    "            try: # testar se existe um livro naquela posição, caso não exista simplesmente passará para o próximo livro\n",
    "                lista_reviews=[]\n",
    "                contagem = 0 # se o try funcionar, foi encontrado um livro, portanto, zerar contador\n",
    "                \n",
    "                # chamar a função dados_iniciais_livro\n",
    "                Livro, Autor, ISBN_13, lancamento, Paginas, total_resenhas, url_resenha = dados_iniciais_livro(i)\n",
    "                \n",
    "                # chamar a função detalhes_do_livro\n",
    "                descricao, generos, Rating, Avaliacoes, total_resenhas_real, abandonos, relendo, querem_ler, lendo, leram, url_livro = detalhes_do_livro(i, Livro) \n",
    "                \n",
    "                # chamar a função da data_de_lancamento\n",
    "                lancamento = data_de_lancamento(i, lancamento)                                             \n",
    "                \n",
    "                if total_resenhas != 0: # caso existam resenhas para este livro\n",
    "                    lista_reviews = obter_resenhas(i, Livro, total_resenhas, limite_resenhas, imprimir)                      \n",
    "                    \n",
    "                else: # caso não exista nenhuma resenha \n",
    "                    lista_reviews = None\n",
    "                # Salvar o livro após coleta\n",
    "                lista_livros.append({'autor':Autor,\n",
    "                                     'titulo':Livro,\n",
    "                                     'numero_do_livro':i,\n",
    "                                     'generos':generos,\n",
    "                                     'data_lancamento':lancamento,\n",
    "                                     'ISBN_13':ISBN_13,\n",
    "                                     'paginas':Paginas,\n",
    "                                     'nota_media':Rating,\n",
    "                                     'total_de_avaliacoes':Avaliacoes,\n",
    "                                     'leram':leram,\n",
    "                                     'lendo':lendo,\n",
    "                                     'querem_ler':querem_ler,\n",
    "                                     'relendo':relendo,\n",
    "                                     'abandonos':abandonos,\n",
    "                                     'total_resenhas':total_resenhas_real,\n",
    "                                     'url_livro':url_livro,\n",
    "                                     'url_resenha':url_resenha,\n",
    "                                     'descricao':descricao,\n",
    "                                     'reviews':lista_reviews\n",
    "                                    })\n",
    "                break\n",
    "                    \n",
    "                    \n",
    "            except IndexError: # caso não exista um livro nesta página, passar para próximo livro\n",
    "                contagem +=1 # adicionar 1 ao contador para cada livro não existente\n",
    "                break\n",
    "                \n",
    "            except requests.exceptions.ConnectionError: # caso dê erro de internet, continuar tentando até a internet voltar\n",
    "                continue        \n",
    "            except requests.exceptions.ChunkedEncodingError: # caso dê erro de internet, continuar tentando até a internet voltar\n",
    "                continue           \n",
    "            except AttributeError:\n",
    "                contagem +=1\n",
    "                break\n",
    "    print('\\n')\n",
    "    print (time.asctime(time.localtime(time.time())))    \n",
    "    return(lista_livros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a03b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "livros = raspar_skoob(1,1000,0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955baa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_livros = pd.DataFrame(livros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641649f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_livros)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_livros.numero_do_livro.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_livros.dropna(subset=['reviews']).numero_do_livro.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_livros.to_csv('df_livros_1_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliar_resenhas = pd.DataFrame(columns=['review_id','user_id','numero_do_livro','pagina_do_skoob','posicao_na_pagina','rating','review_title','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliar_resenhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in df_livros.reviews.iteritems():\n",
    "    auxiliar_resenhas = pd.concat([auxiliar_resenhas,pd.DataFrame(d)],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c870f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auxiliar_resenhas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258112a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_livros.drop('reviews',axis=1).merge(auxiliar_resenhas,on='numero_do_livro',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d20f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acc0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e98f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e3387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
